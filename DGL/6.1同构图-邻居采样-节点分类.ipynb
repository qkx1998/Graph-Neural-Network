{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "591936ca",
   "metadata": {},
   "source": [
    "如果用户有包含数百万甚至数十亿个节点或边的大图，通常无法直接进行全图训练。  \n",
    "考虑在一个有 N 个节点的图上运行的、隐层大小为 H 的 L 层图卷积网络， 存储隐层表示需要 O(NLH) 的内存空间，当 N 较大时，这很容易超过一块GPU的显存限制。  \n",
    "本章介绍了一种在大图上进行**随机小批次训练**的方法，可以让用户不用一次性把所有节点特征拷贝到GPU上。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0379a4a3",
   "metadata": {},
   "source": [
    "### **邻居节点采样的工作流程通常如下**：  \n",
    "\n",
    "每次梯度下降，选择一个小批次的图节点， 其最终表示将在神经网络的第 L 层进行计算，然后在网络的第 L−1 层选择该批次节点的全部或部分邻居节点。  \n",
    "重复这个过程，直到到达输入层。这个迭代过程会构建计算的依赖关系图，从输出开始，一直到输入"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56744da3",
   "metadata": {},
   "source": [
    "#### 定义邻居采样器和数据加载器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41daf22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import dgl.nn as dglnn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "sampler = dgl.dataloading.MultiLayerFullNeighborSampler(2)\n",
    "\n",
    "#以下代码创建了一个PyTorch的 DataLoader，它分批迭代训练节点ID数组 train_nids， 并将生成的子图列表放到GPU上\n",
    "dataloader = dgl.dataloading.NodeDataLoader(\n",
    "        g, train_nids, sampler,\n",
    "        batch_size=1024,\n",
    "        shuffle=True,\n",
    "        drop_last=False,\n",
    "        num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdaef85",
   "metadata": {},
   "source": [
    "对DataLoader进行迭代，将会创建一个特定图的列表，这些图表示每层的计算依赖。在DGL中称之为 块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0de666",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_nodes, output_nodes, blocks = next(iter(dataloader))\n",
    "print(blocks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4dfb37",
   "metadata": {},
   "source": [
    "调整模型以进行小批次训练, 用户所需要做的就是用上面生成的块( block )来替换图( g )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452615ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StochasticTwoLayerGCN(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, out_features):\n",
    "        super().__init__()\n",
    "        self.conv1 = dgl.nn.GraphConv(in_features, hidden_features)\n",
    "        self.conv2 = dgl.nn.GraphConv(hidden_features, out_features)\n",
    "\n",
    "    def forward(self, blocks, x):\n",
    "        x = F.relu(self.conv1(blocks[0], x))\n",
    "        x = F.relu(self.conv2(blocks[1], x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48dcec9e",
   "metadata": {},
   "source": [
    "模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36657a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = StochasticTwoLayerGCN(in_features, hidden_features, out_features)\n",
    "model = model.cuda()\n",
    "opt = torch.optim.Adam(model.parameters())\n",
    "\n",
    "for input_nodes, output_nodes, blocks in dataloader:\n",
    "    blocks = [b.to(torch.device('cuda')) for b in blocks]\n",
    "    input_features = blocks[0].srcdata['features']\n",
    "    output_labels = blocks[-1].dstdata['label']\n",
    "    output_predictions = model(blocks, input_features)\n",
    "    loss = compute_loss(output_labels, output_predictions)\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DGL",
   "language": "python",
   "name": "dgl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
